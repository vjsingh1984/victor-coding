# Code Review Workflow
# ====================
# Comprehensive code review with:
# - Static analysis and linting
# - Security scanning
# - Best practices verification
# - Performance review
# - Documentation check
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ EXECUTION ENVIRONMENT                                                        │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Default: in-process (Python) + subprocess (external tools)                  │
# │ Supported: in-process, subprocess, docker                                    │
# │                                                                              │
# │ Requirements:                                                                │
# │   - Linters: ruff, flake8, eslint (language-dependent)                      │
# │   - Type checkers: mypy, pyright, tsc                                       │
# │   - Security: bandit, semgrep, trivy                                        │
# │   - Complexity: radon, lizard                                               │
# │                                                                              │
# │ Tool installation varies by project - use project's tool config             │
# │ (pyproject.toml, package.json, .tool-versions)                              │
# │                                                                              │
# │ Docker: Useful for CI pipelines with pre-installed tool versions            │
# │   - Image: victor-codereview:latest                                         │
# │   - Build: docker build -f docker/Dockerfile.codereview -t victor-cr .      │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ DEFAULT VALUES                                                               │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ diff_command: git diff HEAD~1 (or git diff main...HEAD)                     │
# │ lint_command: ruff check . (or project-specific)                            │
# │ type_check_command: mypy . (or project-specific)                            │
# │ security_scan_command: bandit -r . (or semgrep, trivy)                      │
# │ complexity_command: radon cc . -a -nc (or lizard)                           │
# │ test_command: pytest (or project-specific)                                  │
# │ hitl_timeout: 900s (15 min for human reviews)                               │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ COMPUTE vs AGENT NODE RATIONALE                                              │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ COMPUTE nodes (no LLM reasoning):                                            │
# │   - gather_changes: git diff/log commands (subprocess)                      │
# │   - lint_check: ruff/eslint subprocess execution                            │
# │   - type_check: mypy/tsc subprocess execution                               │
# │   - security_scan: bandit/semgrep subprocess execution                      │
# │   - complexity_analysis: radon/lizard subprocess execution                  │
# │   - verify_fixes: pytest/test runner subprocess execution                   │
# │   All tool outputs are deterministic based on code - no LLM needed.         │
# │                                                                              │
# │ AGENT nodes (require LLM reasoning):                                         │
# │   - ai_review: INTERPRETING results and finding logic issues                │
# │   - categorize_findings: PRIORITIZING issues by severity                    │
# │   - apply_fixes: WRITING code to fix issues                                 │
# │   - approve_with_comments: SUMMARIZING and WRITING review                   │
# │   All require judgment, interpretation, or creative writing.                │
# └─────────────────────────────────────────────────────────────────────────────┘

workflows:
  code_review:
    description: "Comprehensive automated code review with feedback"

    metadata:
      version: "1.0"
      author: "victor"
      vertical: coding

    # =========================================================================
    # SERVICE DEFINITIONS
    # =========================================================================
    services:
      # Project database for tracking review history
      project_db:
        type: sqlite
        config:
          path: $ctx.project_dir/.victor/project.db
          journal_mode: WAL
        lifecycle:
          start: auto
          cleanup: preserve

    nodes:
      # =====================================================================
      # Stage 1: Gather Changes
      # =====================================================================
      # COMPUTE: git diff/log are subprocess commands with deterministic output
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm: subprocess.run(["git", "diff", ...])
      # Output is text diff - no interpretation, just capture.
      #
      # WHY NOT AGENT: git commands are mechanical - they output text.
      # No reasoning about WHAT changed - that's the agent's job later.
      #
      # Execution: subprocess (git CLI)
      # BLOCKED: [llm, write] - read-only git operation
      - id: gather_changes
        type: compute
        name: "Gather Code Changes"
        tools: [shell]
        inputs:
          command: $ctx.diff_command
          base_branch: $ctx.base_branch
        output: changes
        constraints: [llm, write]
        timeout: 60
        next: [check_changes]

      - id: check_changes
        type: condition
        name: "Check Changes Exist"
        condition: "change_count > 0"
        branches:
          "true": parallel_analysis
          "false": no_changes

      - id: no_changes
        type: transform
        name: "No Changes to Review"
        transform: |
          status = "skipped"
          reason = "No code changes found"

      # =====================================================================
      # Stage 2: Parallel Analysis
      # =====================================================================
      - id: parallel_analysis
        type: parallel
        name: "Run Parallel Checks"
        parallel_nodes: [lint_check, type_check, security_scan, complexity_analysis]
        join_strategy: all
        next: [aggregate_results]

      # COMPUTE: Linters are subprocess tools with rule-based output
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm: ruff/flake8/eslint apply AST rules to code
      # Output is structured: file:line:col: code message
      # Rules are configured in pyproject.toml/.eslintrc - no LLM.
      #
      # Execution: subprocess (ruff, flake8, eslint)
      # BLOCKED: [llm, write] - read-only analysis
      - id: lint_check
        type: compute
        name: "Run Linters"
        tools: [shell]
        inputs:
          commands:
            - $ctx.lint_command
            - $ctx.format_check_command
        output: lint_results
        constraints: [llm, write]
        timeout: 180

      # COMPUTE: Type checkers are static analysis tools
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm: mypy/pyright perform type inference and checking
      # Output is structured errors - deterministic for given code + config.
      #
      # Execution: subprocess (mypy, pyright, tsc)
      # BLOCKED: [llm, write] - read-only analysis
      - id: type_check
        type: compute
        name: "Run Type Checker"
        tools: [shell]
        inputs:
          command: $ctx.type_check_command
        output: type_results
        constraints: [llm, write]
        timeout: 180

      # COMPUTE: Security scanners apply pattern-based detection
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm: bandit/semgrep apply security rules to AST
      # SAST (Static Application Security Testing) - no runtime.
      # Output: vulnerability type, severity, line number.
      #
      # Execution: subprocess (bandit, semgrep, trivy)
      # BLOCKED: [llm, write] - read-only analysis
      - id: security_scan
        type: compute
        name: "Security Analysis"
        tools: [shell]
        inputs:
          commands:
            - $ctx.security_scan_command
        output: security_results
        constraints: [llm, write]
        timeout: 300

      # COMPUTE: Complexity analysis uses AST metrics
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm: radon/lizard compute cyclomatic complexity (CC)
      # CC = edges - nodes + 2p (McCabe formula) - pure graph theory.
      # Also: Halstead metrics, maintainability index.
      #
      # Execution: subprocess (radon, lizard)
      # BLOCKED: [llm, write] - read-only analysis
      - id: complexity_analysis
        type: compute
        name: "Complexity Analysis"
        tools: [shell]
        inputs:
          command: $ctx.complexity_command
        output: complexity_results
        constraints: [llm, write]
        timeout: 120

      # =====================================================================
      # Stage 3: Aggregate and AI Review
      # =====================================================================
      - id: aggregate_results
        type: transform
        name: "Aggregate Analysis Results"
        transform: |
          total_issues = lint_issues + type_issues + security_issues
          has_blocking = security_critical > 0 or type_errors > 0
        next: [ai_review]

      - id: ai_review
        type: agent
        name: "AI Code Review"
        role: reviewer
        goal: |
          Perform a detailed code review focusing on:

          1. **Logic and Correctness**
             - Are there any bugs or logic errors?
             - Are edge cases handled?
             - Are error conditions properly managed?

          2. **Code Quality**
             - Is the code readable and maintainable?
             - Are functions/methods appropriately sized?
             - Is there code duplication?

          3. **Best Practices**
             - Does it follow project conventions?
             - Are design patterns used appropriately?
             - Is there proper separation of concerns?

          4. **Performance**
             - Are there obvious performance issues?
             - Any N+1 queries or expensive loops?
             - Memory usage concerns?

          5. **Testing**
             - Are changes adequately tested?
             - Are test cases meaningful?
             - Is coverage sufficient?

          Analysis Results:
          - Lint Issues: {lint_issues}
          - Type Issues: {type_issues}
          - Security Issues: {security_issues}
          - Complexity Score: {complexity_score}
        tool_budget: 30
        tools: [read, grep, code_search]
        llm_config:
          temperature: 0.4
        output: review_findings
        next: [categorize_findings]

      - id: categorize_findings
        type: agent
        name: "Categorize Review Findings"
        role: analyst
        goal: |
          Categorize review findings by severity:

          **Critical** - Must fix before merge:
          - Security vulnerabilities
          - Data loss risks
          - Breaking changes

          **Major** - Should fix:
          - Bugs and logic errors
          - Missing error handling
          - Performance issues

          **Minor** - Nice to have:
          - Code style improvements
          - Documentation gaps
          - Refactoring suggestions

          **Nitpicks** - Optional:
          - Naming suggestions
          - Comment improvements
        tool_budget: 10
        llm_config:
          temperature: 0.3
        input_mapping:
          findings: review_findings
        output: categorized_findings
        next: [check_blocking]

      # =====================================================================
      # Stage 4: Decision Point
      # =====================================================================
      - id: check_blocking
        type: condition
        name: "Check for Blocking Issues"
        condition: "has_critical_issues"
        branches:
          "true": request_changes
          "false": check_major

      - id: check_major
        type: condition
        name: "Check for Major Issues"
        condition: "major_issue_count > 2"
        branches:
          "true": request_changes
          "false": approve_with_comments

      - id: request_changes
        type: hitl
        name: "Request Changes"
        hitl_type: approval
        prompt: |
          ## Code Review: Changes Requested

          **Summary:** {review_summary}

          ### Critical Issues ({critical_count})
          {critical_issues}

          ### Major Issues ({major_count})
          {major_issues}

          ### Minor Issues ({minor_count})
          {minor_issues}

          ---
          Please address the critical and major issues before re-review.
        context_keys:
          - review_summary
          - critical_count
          - critical_issues
          - major_count
          - major_issues
          - minor_count
          - minor_issues
        timeout: 300
        fallback: continue
        next: [offer_auto_fix]

      - id: offer_auto_fix
        type: hitl
        name: "Offer Auto-Fix"
        hitl_type: input
        prompt: |
          Would you like me to automatically fix the issues I can address?

          **Auto-fixable Issues:**
          {auto_fixable_issues}
        context_keys:
          - auto_fixable_issues
        choices:
          - "Yes, fix automatically"
          - "No, I'll fix manually"
        timeout: 300
        fallback: continue
        next: [handle_auto_fix]

      - id: handle_auto_fix
        type: condition
        name: "Handle Auto-Fix Choice"
        condition: "auto_fix_choice"
        branches:
          "Yes, fix automatically": apply_fixes
          "No, I'll fix manually": complete_with_changes

      - id: apply_fixes
        type: agent
        name: "Apply Auto-Fixes"
        role: executor
        goal: |
          Apply automatic fixes for:
          {auto_fixable_issues}

          Guidelines:
          - Make minimal, targeted changes
          - Preserve existing logic
          - Add comments explaining changes
        tool_budget: 30
        tools: [read, edit, shell]
        llm_config:
          temperature: 0.1
        output: applied_fixes
        next: [verify_fixes]

      # COMPUTE: Test runner verifies fixes with pass/fail output
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm: pytest/jest runs test suite
      # Output is structured: passed, failed, skipped counts + failures.
      # No interpretation - just run tests and capture results.
      #
      # Execution: subprocess (pytest, jest, go test)
      # BLOCKED: [llm] - tests need write for temp files, network for fixtures
      - id: verify_fixes
        type: compute
        name: "Verify Fixes"
        tools: [shell]
        inputs:
          command: $ctx.test_command
        output: fix_verification
        constraints: [llm]
        timeout: 300
        next: [check_fix_success]

      - id: check_fix_success
        type: condition
        name: "Check Fix Success"
        condition: "fixes_verified"
        branches:
          "true": re_review
          "false": complete_with_changes

      - id: re_review
        type: agent
        name: "Quick Re-Review"
        role: reviewer
        goal: "Verify the auto-fixes resolved the issues without introducing new ones."
        tool_budget: 15
        tools: [read, grep]
        output: re_review_result
        next: [approve_with_comments]

      # =====================================================================
      # Stage 5: Approval
      # =====================================================================
      - id: approve_with_comments
        type: agent
        name: "Generate Approval Summary"
        role: writer
        goal: |
          Generate a review approval with:
          - Summary of what was reviewed
          - Positive aspects of the code
          - Minor suggestions (optional)
          - Approval statement
        tool_budget: 10
        llm_config:
          temperature: 0.5
        output: approval_summary
        next: [human_approval]

      - id: human_approval
        type: hitl
        name: "Final Approval"
        hitl_type: approval
        prompt: |
          ## Code Review: Ready for Approval

          **AI Review Summary:**
          {approval_summary}

          **Analysis Results:**
          - Lint: {lint_status}
          - Types: {type_status}
          - Security: {security_status}
          - Complexity: {complexity_score}

          **Remaining Minor Issues:** {minor_count}

          Do you approve this code for merge?
        context_keys:
          - approval_summary
          - lint_status
          - type_status
          - security_status
          - complexity_score
          - minor_count
        choices:
          - "Approve"
          - "Request more changes"
          - "Discuss"
        timeout: 900
        fallback: continue
        next: [handle_approval]

      - id: handle_approval
        type: condition
        name: "Handle Approval Decision"
        condition: "approval_decision"
        branches:
          "Approve": complete_approved
          "Request more changes": complete_with_changes
          "Discuss": schedule_discussion

      - id: schedule_discussion
        type: transform
        name: "Schedule Discussion"
        transform: |
          status = "pending_discussion"
          action_required = "Schedule code review discussion"

      - id: complete_approved
        type: transform
        name: "Review Approved"
        transform: |
          status = "approved"
          approval_time = current_timestamp()

      - id: complete_with_changes
        type: transform
        name: "Changes Requested"
        transform: |
          status = "changes_requested"


  # =========================================================================
  # Quick Review - Lightweight Check
  # =========================================================================
  # Fast-path workflow for small PRs (< 100 lines changed)
  # Uses haiku model for speed, minimal tool budget
  # Execution: in-process + subprocess (git)
  quick_review:
    description: "Quick code review for small changes"

    metadata:
      vertical: coding

    nodes:
      # COMPUTE: git diff is deterministic subprocess
      # BLOCKED: [llm, write] - read-only git operation
      - id: gather
        type: compute
        name: "Gather Changes"
        tools: [shell]
        output: changes
        constraints: [llm, write]
        timeout: 30
        next: [review]

      - id: review
        type: agent
        name: "Quick Review"
        role: reviewer
        goal: |
          Quick review focusing on:
          - Obvious bugs or issues
          - Security concerns
          - Test coverage
        tool_budget: 15
        tools: [read, grep]
        llm_config:
          temperature: 0.4
          model_hint: claude-3-haiku
        output: review
        next: [verdict]

      - id: verdict
        type: condition
        name: "Review Verdict"
        condition: "has_issues"
        branches:
          "true": needs_attention
          "false": approved

      - id: needs_attention
        type: transform
        name: "Needs Attention"
        transform: |
          status = "needs_attention"

      - id: approved
        type: transform
        name: "Approved"
        transform: |
          status = "approved"


  # =========================================================================
  # PR Review - Pull Request Review Workflow
  # =========================================================================
  # Full PR review with impact analysis and dependency checking
  # Execution: in-process + subprocess (git, gh CLI)
  pr_review:
    description: "Pull request review with impact analysis and test coverage"

    metadata:
      version: "1.0"
      author: "victor"
      vertical: coding

    nodes:
      # =====================================================================
      # Stage 1: Fetch PR Changes
      # =====================================================================
      # COMPUTE: git/gh commands are subprocess with structured output
      # ─────────────────────────────────────────────────────────────────────
      # Algorithm: git log, git diff, gh pr view subprocess calls
      # Handler aggregates: commits, diff stats, PR metadata
      # All deterministic based on PR state.
      #
      # Execution: subprocess (git, gh CLI)
      # BLOCKED: [llm, write] - read-only git/GitHub operations
      - id: fetch_pr
        type: compute
        name: "Fetch PR Changes"
        handler: git_pr_fetcher
        tools: [shell, read]
        inputs:
          pr_number: $ctx.pr_number
          base_branch: $ctx.base_branch
          fetch_metadata: true
          fetch_diff: true
          fetch_commits: true
        output: pr_changes
        constraints: [llm, write]
        timeout: 120
        next: [analyze_impact]

      # =====================================================================
      # Stage 2: Analyze Impact
      # =====================================================================
      - id: analyze_impact
        type: agent
        name: "Analyze Impact"
        role: researcher
        goal: |
          Analyze the impact of changes on the codebase:

          1. **Scope Analysis**
             - What parts of the codebase are affected?
             - Are there breaking changes?
             - How many files/functions are modified?

          2. **Dependency Analysis**
             - What depends on the modified code?
             - Could changes break existing functionality?
             - Are there circular dependencies introduced?

          3. **Risk Assessment**
             - Identify high-risk changes
             - Check for potential regressions
             - Flag changes to critical paths

          Changes to analyze: {pr_changes}
        tool_budget: 20
        tools: [read, grep, code_search, references, symbols]
        llm_config:
          temperature: 0.3
        input_mapping:
          changes: pr_changes
        output: impact_analysis
        next: [check_tests]

      # =====================================================================
      # Stage 3: Check Tests
      # =====================================================================
      - id: check_tests
        type: agent
        name: "Check Test Coverage"
        role: reviewer
        goal: |
          Check test coverage for the changes:

          1. **Existing Tests**
             - Are there tests covering the modified code?
             - Do the tests pass with the changes?
             - Is coverage adequate?

          2. **New Tests**
             - Were tests added for new functionality?
             - Do new tests follow project conventions?
             - Are edge cases covered?

          3. **Test Quality**
             - Are tests meaningful (not just coverage padding)?
             - Do tests verify correct behavior?
             - Are there integration tests if needed?
        tool_budget: 15
        tools: [shell, read, grep]
        llm_config:
          temperature: 0.3
        output: test_results
        next: [generate_review]

      # =====================================================================
      # Stage 4: Generate Review
      # =====================================================================
      - id: generate_review
        type: agent
        name: "Generate Review"
        role: planner
        goal: |
          Generate actionable review comments for the pull request.

          Based on the analysis:
          - Impact Analysis: {impact_analysis}
          - Test Results: {test_results}

          Structure your review as:

          ## Summary
          Brief overview of what this PR does and its quality.

          ## Strengths
          - What's done well

          ## Issues to Address
          - **Critical**: Must fix before merge
          - **Major**: Should fix
          - **Minor**: Nice to have

          ## Specific Comments
          For each issue, provide:
          - File and line reference
          - Clear description of the issue
          - Suggested fix or approach

          ## Verdict
          - Approve / Request Changes / Needs Discussion
        tool_budget: 10
        tools: [read]
        llm_config:
          temperature: 0.4
        input_mapping:
          changes: pr_changes
          impact: impact_analysis
          tests: test_results
        output: review_comments
        next: [check_verdict]

      # =====================================================================
      # Stage 5: Verdict Decision
      # =====================================================================
      - id: check_verdict
        type: condition
        name: "Check Review Verdict"
        condition: "review_verdict"
        branches:
          "approved": pr_approved
          "changes_requested": pr_changes_requested
          "needs_discussion": pr_needs_discussion

      - id: pr_approved
        type: hitl
        name: "PR Approved"
        hitl_type: approval
        prompt: |
          ## Pull Request Review: Approved

          **Summary:** {review_summary}

          **Impact:** {impact_summary}

          **Test Coverage:** {test_summary}

          ---
          The PR looks good to merge. Do you want to proceed?
        context_keys:
          - review_summary
          - impact_summary
          - test_summary
        choices:
          - "Approve and Merge"
          - "Review More"
        timeout: 600
        fallback: continue
        next: [complete_approved]

      - id: pr_changes_requested
        type: hitl
        name: "Changes Requested"
        hitl_type: approval
        prompt: |
          ## Pull Request Review: Changes Requested

          **Issues Found:**
          {review_comments}

          **Impact Concerns:**
          {impact_concerns}

          ---
          Please address the issues before the PR can be merged.
        context_keys:
          - review_comments
          - impact_concerns
        timeout: 600
        fallback: continue
        next: [complete_changes]

      - id: pr_needs_discussion
        type: transform
        name: "Needs Discussion"
        transform: |
          status = "needs_discussion"
          action = "Schedule review meeting"

      - id: complete_approved
        type: transform
        name: "PR Approved"
        transform: |
          status = "approved"
          can_merge = true

      - id: complete_changes
        type: transform
        name: "Changes Requested"
        transform: |
          status = "changes_requested"
          can_merge = false
